% This file was created with JabRef 2.6.
% Encoding: UTF8

@ARTICLE{cpso2004,
  author = {Frans {van den Bergh} and Andries P. Engelbrech},
  title = {A Cooperative Approach to Particle Swarm Optimization},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2004},
  volume = {8},
  pages = {225--239},
  number = {3},
  file = {cpso2004.pdf:Evolutionary Computation/Large Scale Optimization/cpso2004.pdf:PDF}
}

@CONFERENCE{de2009swarmCUDA,
  author = {Lucas {de P. Veronese} and Renato A. Krohling},
  title = {{Swarm's flight: accelerating the particles using C-CUDA}},
  booktitle = {Proceedings of the Eleventh conference on Congress on Evolutionary
	Computation},
  year = {2009},
  pages = {3264--3270},
  organization = {Institute of Electrical and Electronics Engineers Inc., The},
  file = {de2009swarmCUDA.pdf:Evolutionary Computation/GPU-based Algorithms/de2009swarmCUDA.pdf:PDF}
}

@CONFERENCE{aickelinPyramidal,
  author = {Aickelin, U.},
  title = {{A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering
	Strategies for Scheduling Problems}},
  booktitle = {GECCO Late-Breaking Papers},
  pages = {1--8},
  file = {aickelinPyramidal.pdf:Evolutionary Computation/PPSN 2010/reference papers/aickelinPyramidal.pdf:PDF}
}

@INCOLLECTION{PPSN2010AMNPAMJ,
  author = {Al Moubayed, Noura and Petrovski, Andrei and McCall, John},
  title = {A Novel Smart Multi-Objective Particle Swarm Optimisation Using Decomposition},
  booktitle = {Parallel Problem Solving from Nature – PPSN XI},
  publisher = {Springer Berlin / Heidelberg},
  year = {2011},
  editor = {Schaefer, Robert and Cotta, Carlos and Kolodziej, Joanna and Rudolph,
	Günter},
  volume = {6239},
  series = {Lecture Notes in Computer Science},
  pages = {1-10},
  note = {10.1007/978-3-642-15871-1_1},
  abstract = {A novel Smart Multi-Objective Particle Swarm Optimisation method -
	SDMOPSO - is presented in the paper. The method uses the decomposition
	approach proposed in MOEA/D, whereby a multi-objective problem (MOP)
	is represented as several scalar aggregation problems. The scalar
	aggregation problems are viewed as particles in a swarm; each particlee
	assigns weights to every optimisation objective. The problem is solved
	then as a Multi-Objective Particle Swarm Optimisation (MOPSO), in
	which every particle uses information from a set of defined neighbours.
	The paper also introduces a novel smart approach for sharing information
	between particles, whereby each particle calculates a new position
	in advance using its neighbourhood information and shares this new
	information with the swarm. The results of applying SDMOPSO on five
	standard MOPs show that SDMOPSO is highly competitive comparing with
	two state-of-the-art algorithms.},
  affiliation = {Robert Gordon University, Aberdeen, St. Andrew Street, Aberdeen, AB25
	1HG UK},
  url = {http://dx.doi.org/10.1007/978-3-642-15871-1_1}
}

@ARTICLE{altenberg1997nk,
  author = {Altenberg, L.},
  title = {{NK fitness landscapes}},
  journal = {Handbook of Evolutionary Computation, B},
  year = {1997},
  volume = {2},
  file = {altenberg1997nk.pdf:Epistasis/Reference Papers/altenberg1997nk.pdf:PDF}
}

@INPROCEEDINGS{restartCMA,
  author = {Ann Auger and Nikolaus Hansen},
  title = {A Restart {{CMA}} Evolution Strategy with Increasing Population Size},
  booktitle = {IEEE Congress on Evolutionary Computation},
  year = {2005},
  volume = {2},
  publisher = {IEEE Press},
  file = {restartCMAES.pdf:Evolutionary Computation/JADE/Related PAPER/restartCMAES.pdf:PDF}
}

@MISC{addSepDef,
  author = {STEVEN F. BELLENOT},
  title = {ADDITIVELY SEPARABLE FUNCTIONS FUNCTIONS OF THE FORM F (x, y) = f
	(x) + g(y)},
  file = {addSepDef.pdf:Evolutionary Computation/JADE/Related PAPER/addSepDef.pdf:PDF},
  owner = {chenwx},
  timestamp = {2010.07.31}
}

@ARTICLE{bellman1961adaptive,
  author = {Bellman, R.},
  title = {{Adaptive control processes: a guided tour}},
  journal = {Princeton University Press, Princeton, New Jersey, USA},
  year = {1961},
  volume = {19},
  pages = {94}
}

@BOOK{B1957DP,
  title = {Dynamic Programming},
  publisher = {Princeton University Press},
  year = {1957},
  author = {Bellman, R.E.},
  address = {Princeton, NJ, USA}
}

@ARTICLE{NCJ2002BHGSHP,
  author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
  title = {Evolution strategies – A comprehensive introduction},
  journal = {Natural Computing},
  year = {2002},
  volume = {1},
  pages = {3-52},
  note = {10.1023/A:1015059928466},
  abstract = {This article gives a comprehensive introduction into one of the main
	branches of evolutionary computation – the evolution strategies (ES)
	the history of which dates back to the 1960s in Germany. Starting
	from a survey of history the philosophical background is explained
	in order to make understandable why ES are realized in the way they
	are. Basic ES algorithms and design principles for variation and
	selection operators as well as theoretical issues are presented,
	and future branches of ES research are discussed.},
  file = {NCJ2002BHGSHP.pdf:Evolutionary Computation/ES/NCJ2002BHGSHP.pdf:PDF},
  issn = {1567-7818},
  issue = {1},
  keyword = {Biomedical and Life Sciences},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1023/A:1015059928466}
}

@INPROCEEDINGS{Brest2008cecss,
  author = {Brest, J. and Zamuda, A. and Boskovic, B. and Maucec, M.S. and Zumer,
	V.},
  title = {High-dimensional real-parameter optimization using Self-Adaptive
	Differential Evolution algorithm with population size reduction},
  year = {2008},
  pages = {2032 -2039},
  month = {jun.},
  abstract = {In this paper we investigate a self-adaptive differential evolution
	algorithm (jDEdynNP-F) where F and CR control parameters are self-adapted
	and a population size reduction method is used. Additionally the
	proposed jDEdynNP-F algorithm uses a mechanism for sign changing
	of F control parameter with some probability based on the fitness
	values of randomly chosen vectors, which are multiplied by the F
	control parameter (scaling factor) in the mutation operation of DE
	algorithm. The performance of the jDEdynNP-F algorithm is evaluated
	on the set of 7 benchmark functions provided for the CECpsila2008
	special session on high-dimensional real-parameter optimization.},
  doi = {10.1109/CEC.2008.4631067},
  file = {Brest2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/Brest2008cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {high-dimensional real-parameter optimization;population size reduction;randomly
	chosen vectors;self-adaptive differential evolution algorithm;evolutionary
	computation;optimisation;random processes;}
}

@PHDTHESIS{group:296:brindle1981,
  author = {Brindle, A.},
  title = {Genetic Algorithms for Function Optimization},
  school = {University of Alberta},
  year = {1981},
  type = {PhD Thesis},
  citeulike-article-id = {431776},
  keywords = {bibtex-import},
  posted-at = {2005-12-09 13:43:29},
  priority = {2}
}

@INPROCEEDINGS{Bullinaria05evolvingneural,
  author = {John A. Bullinaria},
  title = {Evolving neural networks: Is it really worth the effort},
  booktitle = {In 13th European Symposium on Artificial Neural Networks (ESANN},
  year = {2005},
  pages = {267--272},
  file = {Bullinaria05evolvingneural.pdf:Evolutionary Computation/XinYao_Classic/Bullinaria05evolvingneural.pdf:PDF}
}

@BOOK{bäck1997handbook,
  title = {{Handbook of evolutionary computation}},
  publisher = {Taylor \& Francis},
  year = {1997},
  author = {B{\\"a}ck, T. and Fogel, D.B. and Michalewicz, Z.}
}

@MISC{ppsn2010Poster,
  author = {Wenxiang Chen},
  title = {{Large-Scale Global Optimization using Cooperative Coevolution with
	Variable Interaction Learning}},
  howpublished = {Via academic homepage, http://mail.ustc.edu.cn/~chenwx},
  month = {September 13},
  year = {2010},
  file = {ppsn2010Poster.pdf:PPSN/Poster/ppsn2010Poster.pdf:PDF},
  owner = {Wenxiang Chen},
  timestamp = {2010.09.18}
}

@MISC{slides2010PPSN,
  author = {Wenxiang Chen},
  title = {{Variable Interaction Learning in Cooperative Coevolution}},
  howpublished = {Via academic homepage, http://mail.ustc.edu.cn/~chenwx},
  month = {August 14},
  year = {2010},
  file = {slides2010PPSN.pdf:PPSN/beamerYao/slides2010PPSN.pdf:PDF},
  owner = {Wenxiang Chen},
  timestamp = {2010.09.18}
}

@INCOLLECTION{ppsn10Chen,
  author = {Chen, Wenxiang and Weise, Thomas and Yang, Zhenyu and Tang, Ke},
  title = {Large-Scale Global Optimization Using Cooperative Coevolution with
	Variable Interaction Learning},
  booktitle = {Parallel Problem Solving from Nature – PPSN XI},
  publisher = {Springer Berlin / Heidelberg},
  year = {2011},
  editor = {Schaefer, Robert and Cotta, Carlos and Kolodziej, Joanna and Rudolph,
	Günter},
  volume = {6239},
  series = {Lecture Notes in Computer Science},
  pages = {300-309},
  note = {10.1007/978-3-642-15871-1_31},
  abstract = {In recent years, Cooperative Coevolution (CC) was proposed as a promising
	framework for tackling high-dimensional optimization problems. The
	main idea of CC-based algorithms is to discover which decision variables,
	i.e, dimensions, of the search space interact. Non-interacting variables
	can be optimized as separate problems of lower dimensionality. Interacting
	variables must be grouped together and optimized jointly. Early research
	in this area started with simple attempts such as one-dimension based
	and splitting-in-half methods. Later, more efficient algorithms with
	new grouping strategies, such as DECC-G and MLCC, were proposed.
	However, those grouping strategies still cannot sufficiently adapt
	to different group sizes. In this paper, we propose a new CC framework
	named Cooperative Coevolution with Variable Interaction Learning
	(CCVIL), which initially considers all variables as independent and
	puts each of them into a separate group. Iteratively, it discovers
	their relations and merges the groups accordingly. The efficiency
	of the newly proposed framework is evaluated on the set of large-scale
	optimization benchmarks.},
  affiliation = {Nature Inspired Computation and Applications Laboratory, School of
	Computer Science and Technology, University of Science and Technology
	of China},
  file = {ppsn10Chen.pdf:PPSN/ppsn10Chen.pdf:PDF;ppsn10Chen.pdf:PPSN/ppsn2010_submission_66 (revised reference)/ppsn10Chen.pdf:PDF;ppsn10Chen.pdf:PPSN/创新学分/PPSNpaper_WenxiangChen/ppsn10Chen.pdf:PDF},
  url = {http://dx.doi.org/10.1007/978-3-642-15871-1_31}
}

@TECHREPORT{chen2007survey,
  author = {Chen, YP and Yu, T.L. and Sastry, K. and Goldberg, D.E.},
  title = {{A survey of linkage learning techniques in genetic and evolutionary
	algorithms}},
  year = {2007},
  file = {chen2007survey.pdf:Epistasis/Reference Papers/chen2007survey.pdf:PDF},
  journal = {IlliGAL Report},
  publisher = {Citeseer},
  volume = {2007014}
}

@INPROCEEDINGS{Corcoran94usingreal-valued,
  author = {Arthur L. Corcoran and Sandip Sen},
  title = {Using Real-Valued Genetic Algorithms to Evolve Rule Sets for Classification},
  booktitle = {In IEEE-CEC},
  year = {1994},
  pages = {120--124},
  file = {Corcoran94usingreal-valued.pdf:EC_Course/Referenced Paper/Corcoran94usingreal-valued.pdf:PDF}
}

@CONFERENCE{DejongGA-NP,
  author = {De Jong, K.A. and Spears, W.M.},
  title = {{Using genetic algorithms to solve NP-complete problems}},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  pages = {124--132},
  organization = {Citeseer},
  abstract = {A strategy for using Genetic Algorithms (GAs) to
	
	solve NP-complete problems is presented. The key
	
	aspect of the approach taken is to exploit the obser-
	
	vation that, although all NP-complete problems are
	
	equally difficult in a general computational sense,
	
	some have much better GA representations than oth-
	
	ers, leading to much more successful use of GAs on
	
	some NP-complete problems than on others. Since
	
	any NP-complete problem can be mapped into any
	
	other one in polynomial time, the strategy described
	
	here consists of identifying a canonical NP-complete
	
	problem on which GAs work well, and solving other
	
	NP-complete problems indirectly by mapping them
	
	onto the canonical problem. Initial empirical results
	
	are presented which support the claim that the
	
	Boolean Satisfiability Problem (SAT) is a GA-
	
	effective canonical problem, and that other NP-
	
	complete problems with poor GA representations
	
	can be solved efficiently by mapping them first onto
	
	SAT problems.},
  file = {DejongGA-NP.pdf:Evolutionary Computation/DejongGA-NP.pdf:PDF}
}

@INPROCEEDINGS{Dejong2010geccoSlides,
  author = {Kenneth Alan {De Jong}},
  title = {Evolutionary computation: a unified approach[Slides]},
  booktitle = {GECCO '10: Proceedings of the 12th annual conference comp on Geneticand
	evolutionary computation},
  year = {2010},
  pages = {2289--2302},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1830761.1830896},
  file = {Dejong2010geccoSlides.pdf:Evolutionary Computation/Dejong2010geccoSlides.pdf:PDF},
  isbn = {978-1-4503-0073-5},
  location = {Portland, Oregon, USA}
}

@BOOK{DejongEAbook,
  title = {{Evolutionary Computation: A Unified Approach.}},
  publisher = {MIT Press},
  year = {2006},
  author = {Kenneth Alan {De Jong}},
  file = {DejongEAbook.pdf:Evolutionary Computation/DejongEAbook.pdf:PDF}
}

@ARTICLE{SMCB1996DMC,
  author = {Dorigo, Marco and Maniezzo, Vittorio and Colorni, Alberto},
  title = {Ant system: optimization by a colony of cooperating agents},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
	on},
  year = {1996},
  volume = {26},
  pages = {29 -41},
  number = {1},
  month = {feb.},
  abstract = {An analogy with the way ant colonies function has suggested the definition
	of a new computational paradigm, which we call ant system (AS). We
	propose it as a viable new approach to stochastic combinatorial optimization.
	The main characteristics of this model are positive feedback, distributed
	computation, and the use of a constructive greedy heuristic. Positive
	feedback accounts for rapid discovery of good solutions, distributed
	computation avoids premature convergence, and the greedy heuristic
	helps find acceptable solutions in the early stages of the search
	process. We apply the proposed methodology to the classical traveling
	salesman problem (TSP), and report simulation results. We also discuss
	parameter selection and the early setups of the model, and compare
	it with tabu search and simulated annealing using TSP. To demonstrate
	the robustness of the approach, we show how the ant system (AS) can
	be applied to other optimization problems like the asymmetric traveling
	salesman, the quadratic assignment and the job-shop scheduling. Finally
	we discuss the salient characteristics-global data structure revision,
	distributed communication and probabilistic transitions of the AS},
  doi = {10.1109/3477.484436},
  file = {SMCB1996DMC.pdf:Evolutionary Computation/Ant Colony Optimization/SMCB1996DMC.pdf:PDF},
  issn = {1083-4419},
  keywords = {ant colonies;ant system;computational paradigm;constructive greedy
	heuristic;cooperating agents;distributed communication;distributed
	computation;global data structure revision;job-shop scheduling;optimization;parameter
	selection;positive feedback;probabilistic transitions;quadratic assignment;simulated
	annealing;stochastic combinatorial optimization;tabu search;traveling
	salesman problem;feedback;optimisation;robust control;search problems;simulated
	annealing;stochastic programming;travelling salesman problems;}
}

@CONFERENCE{duque2009new,
  author = {Duque, T.S.P.C. and Goldberg, D.E.},
  title = {{A new method for linkage learning in the ECGA}},
  booktitle = {Proceedings of the 11th Annual conference on Genetic and evolutionary
	computation},
  year = {2009},
  pages = {1819--1820},
  organization = {ACM},
  file = {duque2009new.pdf:Epistasis/Reference Papers/duque2009new.pdf:PDF}
}

@ARTICLE{duque2008enhancing,
  author = {Thyago S. P. C. Duque and David E. Goldberg and Kumara Sastry},
  title = {{Enhancing the Efficiency of the ECGA}},
  journal = {Parallel Problem Solving from Nature--PPSN X},
  year = {2008},
  pages = {165--174},
  file = {duque2008enhancing.pdf:Epistasis/Reference Papers/duque2008enhancing.pdf:PDF},
  publisher = {Springer}
}

@ARTICLE{SCI1993FS,
  author = {Stephanie Forrest},
  title = {{Genetic Algorithms: Principles of Natural Selection Applied to Computation}},
  journal = {Science},
  year = {1993},
  volume = {261},
  pages = {872-878},
  month = aug,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/1993Sci...261..872F},
  doi = {10.1126/science.8346439},
  file = {SCI1993FS.pdf:Evolutionary Computation/SCI1993FS.pdf:PDF}
}

@BOOK{534133,
  title = {Genetic Algorithms in Search, Optimization and Machine Learning},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  year = {1989},
  author = {Goldberg, David E.},
  address = {Boston, MA, USA},
  isbn = {0201157675}
}

@INPROCEEDINGS{Goldberg91acomparative,
  author = {David E. Goldberg and Kalyanmoy Deb},
  title = {A comparative analysis of selection schemes used in genetic algorithms},
  booktitle = {Foundations of Genetic Algorithms},
  year = {1991},
  pages = {69--93},
  publisher = {Morgan Kaufmann},
  file = {Goldberg91acomparative.pdf:EC_Course/Referenced Paper/Goldberg91acomparative.pdf:PDF}
}

@ARTICLE{gould2005numerical,
  author = {Gould, N. and Orban, D. and Toint, P.},
  title = {{Numerical methods for large-scale nonlinear optimization}},
  journal = {Acta Numerica},
  year = {2005},
  volume = {14},
  pages = {299--361},
  file = {gould2005numerical.pdf:Evolutionary Computation/Large Scale Optimization/gould2005numerical.pdf:PDF},
  publisher = {Cambridge Univ Press}
}

@ARTICLE{hansen2008cma,
  author = {Nikolaus Hansen},
  title = {{The CMA evolution strategy: A tutorial}},
  year = {2005},
  file = {hansen2008cma.pdf:Evolutionary Computation/CMA-ES/hansen2008cma.pdf:PDF}
}

@INPROCEEDINGS{hansen2004evalCMA,
  author = {Nikolaus Hansen and Stefan Kern},
  title = {Evaluating the {{CMA}} Evolution Strategy on Multimodal Test Functions},
  booktitle = {Parallel Problem Solving from Nature -- PPSN VIII},
  year = {2004},
  pages = {282--291},
  publisher = {Springer},
  file = {hansen2004evalCMA.pdf:Evolutionary Computation/CMA-ES/hansen2004evalCMA.pdf:PDF}
}

@ARTICLE{hansen2003reduceCMA,
  author = {Nikolaus Hansen and Sibylle D. M{\"u}ller and Petros Koumoutsakos},
  title = {Reducing the Time Complexity of the Derandomized Evolution Strategy
	with Covariance Matrix Adaptation ({{CMA-ES}})},
  journal = {Evolutionary Computation},
  year = {2003},
  volume = {11},
  pages = {1--18},
  number = {1},
  file = {hansen2003reduceCMA.pdf:Evolutionary Computation/CMA-ES/hansen2003reduceCMA.pdf:PDF}
}

@TECHREPORT{Harik99linkagelearning,
  author = {Georges Harik},
  title = {Linkage Learning via Probabilistic Modeling in the {ECGA}},
  institution = {Illinois Genetic Algorithms Laboratory},
  year = {1999},
  abstract = {The goal of linkage learning, or building block identification, is
	the creation of a
	
	more effective genetic algorithm (GA). This paper explores the relationship
	between the
	
	linkage-learning problem and that of learning probability distributions
	over multi-variate
	
	spaces. Herein, it is argued that these problems are equivalent. Using
	a simple but effective
	
	approach to learning distributions, and by implication linkage, this
	paper reveals the
	
	existence of GA-like algorithms that are potentially orders of magnitude
	faster and more
	
	accurate than the simple GA.},
  file = {Harik99linkagelearning.pdf:Epistasis/Reference Papers/Harik99linkagelearning.pdf:PDF},
  review = {This paper is a quite inspiring paper. Actually, it has the similar
	basic idea as my PPSN paper, yet the viewpoint and the solution for
	the problem is different.
	
	
	Key Points:
	
	1. The complexity of MPM model search is potentially computionally
	expensive. 
	
	A: Look for heuristic approximations to learning algorithms.
	
	
	2. Baysian Network is more powerful than MPM in ECGA, but the computational
	effort will be even higher.}
}

@PHDTHESIS{HarikLinkagePHD,
  author = {Georges Raif Harik},
  title = {Learning gene linkage to efficiently solve problems of bounded difficulty
	using genetic algorithms},
  year = {1997},
  address = {Ann Arbor, MI, USA},
  file = {HarikLinkagePHD.pdf:Epistasis/Reference Papers/HarikLinkagePHD.pdf:PDF},
  order_no = {UMI Order No. GAX97-32090},
  publisher = {University of Michigan}
}

@ARTICLE{Hsieh2009smcb,
  author = {Sheng-Ta Hsieh and Tsung-Ying Sun and Chan-Cheng Liu and Shang-Jeng
	Tsai},
  title = {Efficient Population Utilization Strategy for Particle Swarm Optimizer},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2009},
  volume = {39},
  pages = {444 -456},
  number = {2},
  month = {4},
  abstract = {The particle swarm optimizer (PSO) is a population-based optimization
	technique that can be applied to a wide range of problems. This paper
	presents a variation on the traditional PSO algorithm, called the
	efficient population utilization strategy for PSO (EPUS-PSO), adopting
	a population manager to significantly improve the efficiency of PSO.
	This is achieved by using variable particles in swarms to enhance
	the searching ability and drive particles more efficiently. Moreover,
	sharing principals are constructed to stop particles from falling
	into the local minimum and make the global optimal solution easier
	found by particles. Experiments were conducted on unimodal and multimodal
	test functions such as quadric, griewanks, rastrigin, ackley, and
	weierstrass, with and without coordinate rotation. The results show
	good performance of the EPUS-PSO in solving most benchmark problems
	as compared to other recent variants of the PSO.},
  doi = {10.1109/TSMCB.2008.2006628},
  file = {Hsieh2009smcb.pdf:Evolutionary Computation/Large Scale Optimization/Hsieh2009smcb.pdf:PDF},
  issn = {1083-4419},
  keywords = {global optimal solution;particle swarm optimizer;population utilization
	strategy;population-based optimization technique;searching ability;sharing
	principals;particle swarm optimisation;},
  owner = {Wenxiang Chen},
  timestamp = {2010.08.20}
}

@INPROCEEDINGS{Hsieh2007cecss,
  author = {Sheng-Ta Hsieh and Tsung-Ying Sun and Chan-Cheng Liu and Shang-Jeng
	Tsai},
  title = {Solving large scale global optimization using improved Particle Swarm
	Optimizer},
  year = {2008},
  pages = {1777 -1784},
  month = {jun.},
  abstract = {As more and more real-world optimization problems become increasingly
	complex, algorithms with more capable optimizations are also increasing
	in demand. For solving large scale global optimization problems,
	this paper presents a variation on the traditional PSO algorithm,
	called the efficient population utilization strategy for particle
	swarm optimizer (EPUS-PSO). This is achieved by using variable particles
	in swarms to enhance the searching ability and drive particles more
	efficiently. Moreover, sharing principals are constructed to stop
	particles from falling into the local minimum and make the global
	optimal solution easier found by particles. Experiments were conducted
	on 7 CEC 2008 test functions to present solution searching ability
	of the proposed method.},
  doi = {10.1109/CEC.2008.4631030},
  file = {Hsieh2007cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/Hsieh2007cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {efficient population utilization strategy;improved particle swarm
	optimizer;large scale global optimization;searching ability;variable
	particles;particle swarm optimisation;search problems;}
}

@ARTICLE{hu2008evolvability,
  author = {Hu, T. and Banzhaf, W.},
  title = {{Evolvability and Acceleration in Evolutionary Computation}},
  year = {2008},
  file = {hu2008evolvability.pdf:Epistasis/Reference Papers/hu2008evolvability.pdf:PDF}
}

@INPROCEEDINGS{APSIS2006HTMAS,
  author = {Huang, T. and Mohan, A.S.},
  title = {A Novel Micro-Particle Swarm Optimizer for Solving High Dimensional
	Optimization Problems},
  year = {2006},
  pages = {3535 -3538},
  month = {jul.},
  abstract = {This paper proposes the use of muPSO for solving high dimensional
	optimization problems. As the majority of the computational cost
	for solving optimization problems is spent on evaluating the fitness
	of potential solutions, which are dominated by the population size,
	the small population size of the proposed muPSO can reduce this cost
	significantly while providing an excellent optimization performance.
	The simulations results have confirmed the effectiveness of the proposed
	muPSO},
  doi = {10.1109/APS.2006.1711381},
  file = {APSIS2006HTMAS.pdf:Evolutionary Computation/Large Scale Optimization/APSIS2006HTMAS.pdf:PDF},
  journal = {Antennas and Propagation Society International Symposium 2006, IEEE},
  keywords = {computational cost;high dimensional optimization problems;micro-particle
	swarm optimizer;muPSO;population size;particle swarm optimisation;}
}

@INPROCEEDINGS{gecco2009huang,
  author = {Huang, Yuan-Wei and Chen, Ying-ping},
  title = {On the detection of general problem structures by using inductive
	linkage identification},
  booktitle = {GECCO '09: Proceedings of the 11th Annual conference on Genetic and
	evolutionary computation},
  year = {2009},
  pages = {1853--1854},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1569901.1570200},
  file = {gecco2009huang.pdf:Epistasis/Reference Papers/gecco2009huang.pdf:PDF},
  isbn = {978-1-60558-325-9},
  location = {Montreal, Qu\'{e}bec, Canada}
}

@CONFERENCE{icga91Husbands,
  author = {Husbands, P. and Mill, F.},
  title = {{Simulated co-evolution as the mechanism for emergent planning and
	scheduling}},
  booktitle = {4th Intl.\ Conf.\ on Genetic Algorithms},
  year = {1991},
  pages = {264--270},
  organization = {Morgan Kaufmann},
  file = {icga91Husbands.pdf:Evolutionary Computation/PPSN 2010/reference papers/icga91Husbands.pdf:PDF}
}

@CONFERENCE{iorio2004CC,
  author = {Iorio, A.W. and Li, X.},
  title = {{A cooperative coevolutionary multiobjective algorithm using non-dominated
	sorting}},
  booktitle = {Genetic and Evolutionary Computation--GECCO 2004},
  year = {2004},
  pages = {537--548},
  organization = {Springer},
  file = {iorio2004CC.pdf:Evolutionary Computation/JADE/Related PAPER/Mergement Related Paper/iorio2004CC.pdf:PDF}
}

@INPROCEEDINGS{ICNN1995KJER,
  author = {Kennedy, J. and Eberhart, R.},
  title = {Particle swarm optimization},
  year = {1995},
  volume = {4},
  pages = {1942 -1948 vol.4},
  month = {nov.},
  abstract = {A concept for the optimization of nonlinear functions using particle
	swarm methodology is introduced. The evolution of several paradigms
	is outlined, and an implementation of one of the paradigms is discussed.
	Benchmark testing of the paradigm is described, and applications,
	including nonlinear function optimization and neural network training,
	are proposed. The relationships between particle swarm optimization
	and both artificial life and genetic algorithms are described},
  doi = {10.1109/ICNN.1995.488968},
  file = {ICNN1995KJER.pdf:Evolutionary Computation/PSO/ICNN1995KJER.pdf:PDF},
  journal = {Neural Networks, 1995. Proceedings., IEEE International Conference
	on},
  keywords = {artificial life;evolution;genetic algorithms;multidimensional search;neural
	network;nonlinear functions;optimization;particle swarm;simulation;social
	metaphor;artificial intelligence;genetic algorithms;neural nets;search
	problems;simulation;}
}

@ARTICLE{JSP1984KS,
  author = {Kirkpatrick, Scott},
  title = {Optimization by simulated annealing: Quantitative studies},
  journal = {Journal of Statistical Physics},
  year = {1984},
  volume = {34},
  pages = {975-986},
  note = {10.1007/BF01009452},
  abstract = {Simulated annealing is a stochastic optimization procedure which is
	widely applicable and has been found effective in several problems
	arising in computeraided circuit design. This paper derives the method
	in the context of traditional optimization heuristics and presents
	experimental studies of its computational efficiency when applied
	to graph partitioning and traveling salesman problems.},
  affiliation = {IBM Research 10598 Yorktown Heights New York},
  issn = {0022-4715},
  issue = {5},
  keyword = {Physics and Astronomy},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/BF01009452}
}

@INCOLLECTION{KumarRay2010ABES,
  author = {Hemant Singh Kumar and Tapabrata Ray},
  title = {Divide and Conquer in Coevolution: A Difficult Balancing Act},
  booktitle = {Agent-Based Evolutionary Search},
  publisher = {Springer Berlin Heidelberg},
  year = {2010},
  editor = {Hiot, Lim Meng and Ong, Yew Soon and Sarker, Ruhul Amin and Ray,
	Tapabrata},
  volume = {5},
  series = {Evolutionary Learning and Optimization},
  pages = {117-138},
  note = {10.1007/978-3-642-13425-8_6},
  abstract = {In recent years, Cooperative Coevolutionary Evolutionary Algorithms
	(CCEAs) have been developed as extensions to traditional Evolutionary
	Algorithms (EAs). CCEAs attempt to solve the optimization problems
	by decomposing them into subcomponents referred to as collaborators.
	CCEAs have been deemed attractive for certain complex problems (with
	high number of decision variables), as they can achieve better fitness
	values than traditional EAs by employing divide and conquer strategy.
	However, their performance can vary from good to bad depending on
	the choice of collaborators, separability of problem and the underlying
	recombination scheme. This chapter highlights that a basic CCEA is
	inadequate to handle a wide variety of problems. Thereafter, a CCEA
	with adaptive partitioning (CCEA-AVP) has been introduced, which
	attempts to chose the collaborators adaptively during the search,
	depending on the relationships between the design variables. Studies
	have been done on various test functions and the proposed technique
	has been compared with conventional EA as well as conventional CCEA
	to highlight the benefits. A number of areas of further research
	in CCEA are highlighted to fully exploit the benefits of coevolution.},
  affiliation = {University of New South Wales at Australian Defence Force Academy (UNSW@ADFA)
	School of Engineering and Information Technology Canberra ACT Australia},
  file = {KumarRay2010ABES.pdf:Evolutionary Computation/Large Scale Optimization/KumarRay2010ABES.pdf:PDF},
  isbn = {978-3-642-13425-8},
  keyword = {Engineering},
  url = {http://dx.doi.org/10.1007/978-3-642-13425-8_6}
}

@CONFERENCE{maneeratana2004multi,
  author = {Kuntinee Maneeratana, Kittipong Boonlong and Nachol Chaiyaratana},
  title = {{Multi-objective Optimisation by Co-operative Co-evolution}},
  booktitle = {Parallel Problem Solving from Nature-PPSN VIII},
  year = {2004},
  pages = {772--781},
  organization = {Springer},
  file = {maneeratana2004multi.pdf:Evolutionary Computation/Mutil-Objective/maneeratana2004multi.pdf:PDF}
}

@INCOLLECTION{PPSN2010LJSD,
  author = {L\:{a}ssig, J\:{o}rg and Sudholt, Dirk},
  title = {General Scheme for Analyzing Running Times of Parallel Evolutionary
	Algorithms},
  booktitle = {Parallel Problem Solving from Nature – PPSN XI},
  publisher = {Springer Berlin / Heidelberg},
  year = {2011},
  editor = {Schaefer, Robert and Cotta, Carlos and Kolodziej, Joanna and Rudolph,
	Günter},
  volume = {6238},
  series = {Lecture Notes in Computer Science},
  pages = {234-243},
  note = {10.1007/978-3-642-15844-5_24},
  abstract = {We present new methods for the running time analysis of parallel evolutionary
	algorithms with spatially structured populations. These methods are
	applied to estimate the speed-up gained by parallelization in pseudo-Boolean
	optimization. The possible speed-up increases with the density of
	the topology. Surprisingly, even sparse topologies like ring graphs
	lead to a significant speed-up for many functions while not increasing
	the total number of function evaluations. We also give practical
	hints towards choosing the minimum number of processors that gives
	an optimal speed-up.},
  affiliation = {International Computer Science Institute, Berkeley, CA 94704, USA},
  file = {PPSN2010LJSD.pdf:Evolutionary Computation/Theory/PPSN2010LJSD.pdf:PDF},
  url = {http://dx.doi.org/10.1007/978-3-642-15844-5_24}
}

@ARTICLE{tecLeeY04,
  author = {Chang-Yong Lee and Xin Yao},
  title = {Evolutionary programming using mutations based on the L\'{e}vy probability
	distribution},
  journal = {IEEE Transaction on Evolutionary Computation},
  year = {2004},
  volume = {8},
  pages = {1-13},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  file = {tecLeeY04.pdf:Evolutionary Computation/XinYao_Classic/tecLeeY04.pdf:PDF}
}

@INPROCEEDINGS{LiKangWu2003cec,
  author = {Jhingzi Li and Lishan Kang and Zhijian Wu},
  title = {An adaptive neighborhood-based multi-parent crossover operator for
	real-coded genetic algorithms},
  booktitle = {Evolutionary Computation, 2003. CEC '03. The 2003 Congress on},
  year = {2003},
  volume = {1},
  pages = { 14 - 21 Vol.1},
  month = {8-12},
  abstract = {In this paper, we propose the adaptive neighborhood-based multi-parent
	cross-over operator (ANMX), a novel search operator for real-coded
	genetic algorithms, which generates offspring by the linear non-convex
	combination of several relative-parents, each of which is uniformly
	sampled in the respective neighborhoods of randomly selected parents
	in the population. ANMX is a general variation operator in essence,
	for it not only takes on the form of a multi-parent crossover operator
	but also implicitly has some of the characteristics of a mutation
	operator. To enhance the efficiency, we introduce a mechanism for
	adapting the range of neighborhoods according to the evolutionary
	progress. Numerical experiments using a suite of test functions,
	which are widely studied in the field of evolutionary computation,
	show good search ability of the proposed operator for functions with
	multimodality and/or epistasis.},
  doi = {10.1109/CEC.2003.1299551},
  file = {LiKangWu2003cec.pdf:EC_Course/Referenced Paper/LiKangWu2003cec.pdf:PDF},
  issn = { },
  keywords = {adaptive neighborhood-based multiparent crossover operator; epistasis;
	evolutionary computation; general variation operator; linear nonconvex
	combination; multimodality; mutation operator; offspring generation;
	parent selection; real-coded genetic algorithms; relative parents;
	search operator; test functions; genetic algorithms; search problems;}
}

@ARTICLE{liang2006NC,
  author = {Liang, J. and Baskar, S. and Suganthan, P. and Qin, A.},
  title = {Performance Evaluation of Multiagent Genetic Algorithm},
  journal = {Natural Computing},
  year = {2006},
  volume = {5},
  pages = {83-96},
  note = {10.1007/s11047-005-1625-y},
  abstract = {Zhong et al. (2004 [IEEE Trans. on Systems, Man and Cybernetics (Part
	B), 34: 1128–1141]) proposed the multiagent genetic algorithm (MAGA)
	in their publication titled A multiagent genetic algorithm for global
	numerical optimization . The MAGA exploits the known characteristics
	of some benchmark functions to achieve outstanding results. For example,
	the MAGA exploits the fact that all variables have the same numerical
	value at the global optimum and the same upper and lower bounds to
	solve several 100 dimensional and 1000 dimensional benchmark problems
	with high precision requiring on average 7000 and 16,000 function
	evaluations respectively. In this paper, we evaluate the performance
	of the MAGA experimentally1 and demonstrate that the performance
	of the MAGA significantly deteriorates when the relative positions
	of the variables at the global optimal point are shifted with respect
	to the search ranges.},
  affiliation = {Nanyang Technological University School of Electrical and Electronic
	Engineering Singapore 639798 Singapore Singapore 639798 Singapore},
  file = {liang2006NC.pdf:Evolutionary Computation/Large Scale Optimization/liang2006NC.pdf:PDF},
  issn = {1567-7818},
  issue = {1},
  keyword = {Biomedical and Life Sciences},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s11047-005-1625-y}
}

@ARTICLE{liang2006performance,
  author = {Liang, JJ and Baskar, S. and Suganthan, PN and Qin, AK},
  title = {{Performance evaluation of multiagent genetic algorithm}},
  journal = {Natural Computing},
  year = {2006},
  volume = {5},
  pages = {83--96},
  number = {1},
  file = {liang2006performance.pdf:Epistasis/Reference Papers/liang2006performance.pdf:PDF},
  publisher = {Springer}
}

@CONFERENCE{liang2005novelBenchmark,
  author = {Liang, JJ and Suganthan, PN and Deb, K.},
  title = {{Novel composition test functions for numerical global optimization}},
  booktitle = {Proceedings 2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005},
  year = {2005},
  pages = {68--75},
  file = {liang2005novelBenchmark.pdf:Evolutionary Computation/liang2005novelBenchmark.pdf:PDF}
}

@ARTICLE{liu2005fuzzy,
  author = {Liu, J. and Lampinen, J.},
  title = {A Fuzzy Adaptive Differential Evolution Algorithm},
  journal = {Soft Computing - A Fusion of Foundations, Methodologies and Applications},
  year = {2005},
  volume = {9},
  pages = {448-462},
  note = {10.1007/s00500-004-0363-x},
  abstract = {The differential evolution algorithm is a floating-point encoded evolutionary
	algorithm for global optimization over continuous spaces. The algorithm
	has so far used empirically chosen values for its search parameters
	that are kept fixed through an optimization process. The objective
	of this paper is to introduce a new version of the Differential Evolution
	algorithm with adaptive control parameters – the fuzzy adaptive differential
	evolution algorithm, which uses fuzzy logic controllers to adapt
	the search parameters for the mutation operation and crossover operation.
	The control inputs incorporate the relative objective function values
	and individuals of the successive generations. The emphasis of this
	paper is analysis of the dynamics and behavior of the algorithm.
	Experimental results, provided by the proposed algorithm for a set
	of standard test functions, outperformed those of the standard differential
	evolution algorithm for optimization problems with higher dimensionality.},
  affiliation = {Laboratory of Information Processing Lappeenranta University of Technology
	P.O. Box 20 FIN-53851 Lappeenranta Finland},
  file = {liu2005fuzzy.pdf:Evolutionary Computation/Large Scale Optimization/liu2005fuzzy.pdf:PDF},
  issn = {1432-7643},
  issue = {6},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  review = {1. Fuzzy control that applied in FADE is used for adapting the control
	parameter $CR$ and $F$
	
	2. The so-call high dimension is only up to 50, which can be hardly
	considered as large-scale problem.
	
	3. The experimental result is only compared with naive DE, which can't
	necessarily confirm the strength of FADE over other the state of
	the art DE variant.},
  url = {http://dx.doi.org/10.1007/s00500-004-0363-x}
}

@INPROCEEDINGS{FEPCC,
  author = {Yong Liu and Xin Yao and Qiangu Zhao and Tetsuya Higuchi},
  title = {{Scaling Up Fast Evolutionary Programming with Cooperative Coevolution}},
  booktitle = {IEEE Congress on Evolutionary Computation},
  year = {2001},
  volume = {2},
  pages = {1101--1108},
  publisher = {IEEE Press},
  file = {FEPCC.pdf:Evolutionary Computation/Large Scale Optimization/FEPCC.pdf:PDF}
}

@ARTICLE{COA2003LS,
  author = {Locatelli, Marco and Schoen, Fabio},
  title = {Efficient Algorithms for Large Scale Global Optimization: Lennard-Jones
	Clusters},
  journal = {Computational Optimization and Applications},
  year = {2003},
  volume = {26},
  pages = {173-190},
  note = {10.1023/A:1025798414605},
  abstract = {A stochastic global optimization method is applied to the challenging
	problem of finding the minimum energy conformation of a cluster of
	identical atoms interacting through the Lennard-Jones potential.
	The method proposed incorporates within an already existing and quite
	successful method, monotonic basin hopping, a two-phase local search
	procedure which is capable of significantly enlarging the basin of
	attraction of the global optimum. The experiments reported confirm
	the considerable advantages of this approach, in particular for all
	those cases which are considered in the literature as the most challenging
	ones, namely 75, 98, 102 atoms. While being capable of discovering
	all putative global optima in the range considered, the method proposed
	improves by more than two orders of magnitude the speed and the percentage
	of success in finding the global optima of clusters of 75, 98, 102
	atoms.},
  file = {COA2003LS.pdf:Evolutionary Computation/Large Scale Optimization/COA2003LS.pdf:PDF},
  issn = {0926-6003},
  issue = {2},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1023/A:1025798414605}
}

@ARTICLE{lush1935progeny,
  author = {Lush, J.L.},
  title = {{Progeny test and individual performance as indicators of an animal's
	breeding value}},
  journal = {Journal of Dairy Science},
  year = {1935},
  volume = {18},
  pages = {1--19},
  number = {1},
  file = {lush1935progeny.pdf:Epistasis/Reference Papers/lush1935progeny.pdf:PDF},
  publisher = {Elsevier}
}

@INPROCEEDINGS{MacNish2008cecss,
  author = {MacNish, C. and Xin Yao},
  title = {Direction matters in high-dimensional optimisation},
  year = {2008},
  pages = {2372 -2379},
  month = {jun.},
  abstract = {Directional biases are evident in many benchmarking problems for real-valued
	global optimisation, as well as many of the evolutionary and allied
	algorithms that have been proposed for solving them. It has been
	shown that directional biases make some kinds of problems easier
	to solve for similarly biased algorithms, which can give a misleading
	view of algorithm performance. In this paper we study the effects
	of directional bias for high- dimensional optimisation problems.
	We show that the impact of directional bias is magnified as dimension
	increases, and can in some cases lead to differences in performance
	of many orders of magnitude. We present a new version of the classical
	evolutionary programming algorithm, which we call unbiased evolutionary
	programming (UEP), and show that it has markedly improved performance
	for high-dimensional optimisation.},
  doi = {10.1109/CEC.2008.4631115},
  file = {MacNish2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/MacNish2008cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {benchmarking problems;classical evolutionary programming algorithm;directional
	biases;high-dimensional optimisation;real-valued global optimisation;unbiased
	evolutionary programming;evolutionary computation;}
}

@ARTICLE{euclid.aoms,
  author = {Henry B. Mann and Donald R. Whitney},
  title = {On a Test of whether One of Two Random Variables is Stochastically
	Larger than the Other},
  journal = {The Annals of Mathematical Statistics},
  year = {1947},
  volume = {18},
  pages = {50--60},
  number = {1},
  abstract = {Let $x$ and $y$ be two random variables with continuous cumulative
	distribution functions $f$ and $g$. A statistic $U$ depending on
	the relative ranks of the $x$'s and $y$'s is proposed for testing
	the hypothesis $f = g$. Wilcoxon proposed an equivalent test in the
	Biometrics Bulletin, December, 1945, but gave only a few points of
	the distribution of his statistic. Under the hypothesis $f = g$ the
	probability of obtaining a given $U$ in a sample of $n x's$ and $m
	y's$ is the solution of a certain recurrence relation involving $n$
	and $m$. Using this recurrence relation tables have been computed
	giving the probability of $U$ for samples up to $n = m = 8$. At this
	point the distribution is almost normal. From the recurrence relation
	explicit expressions for the mean, variance, and fourth moment are
	obtained. The 2rth moment is shown to have a certain form which enabled
	us to prove that the limit distribution is normal if $m, n$ go to
	infinity in any arbitrary manner. The test is shown to be consistent
	with respect to the class of alternatives $f(x) > g(x)$ for every
	$x$.},
  address = {Beachwood, OH, USA},
  doi = {10.1214/aoms/1177730491},
  file = {euclid.aoms.pdf:Epistasis/Reference Papers/euclid.aoms.pdf:PDF},
  publisher = {Institute of Mathematical Statistics}
}

@TECHREPORT{pelikan2009performance,
  author = {Martin Pelikan, Kumara Sastry, David E. Goldberg, Martin V. Butz,
	and Mark Hauschild},
  title = {{Performance of evolutionary algorithms on nk landscapes with nearest
	neighbor interactions and tunable overlap (MEDAL Report No. 2009002)}},
  institution = {Missouri Estimation of Distribution Algorithms Laboratory, University
	of Missour--St. Louis, St. Louis, MO},
  year = {2009},
  file = {pelikan2009performance.pdf:Epistasis/Reference Papers/pelikan2009performance.pdf:PDF},
  journal = {Missouri Estimation of Distribution Algorithms Laboratory, University
	of Missour--St. Louis, St. Louis, MO}
}

@ARTICLE{NumericalLSGO,
  author = {Alexander Martin},
  title = {LARGE-SCALE OPTIMIZATION},
  journal = {OPTIMIZATION AND OPERATIONS RESEARCH},
  file = {NumericalLSGO.pdf:Evolutionary Computation/Large Scale Optimization/NumericalLSGO.pdf:PDF},
  owner = {chenwx},
  timestamp = {2010.07.31}
}

@MISC{Mengshoel98deceptiveand,
  author = {Ole J. Mengshoel and David E. Goldberg and David C. Wilkins},
  title = {Deceptive and Other Functions of Unitation as Bayesian Networks},
  year = {1998},
  file = {Mengshoel98deceptiveand.pdf:Epistasis/Reference Papers/Mengshoel98deceptiveand.pdf:PDF}
}

@BOOK{GADSEP1996MZ,
  title = {Genetic Algorithms + Data Structures = Evolution Programs (3rd)},
  publisher = {Springer-Verlag New York, Inc.},
  year = {1996},
  author = {Michalewicz, Zbigniew},
  address = {New York, NY, USA},
  file = {GADSEP1996MZ.PDF:Evolutionary Computation/GADSEP1996MZ.PDF:PDF},
  isbn = {3-540-58090-5}
}

@CONFERENCE{naudts1996epistasis,
  author = {Naudts, B. and Verschoren, A. and Antwerp, B.},
  title = {Epistasis on finite and infinite spaces},
  booktitle = {8th Int. Conf. on Systems Research, Informatics and Cybernetics},
  year = {1996},
  pages = {19--23},
  file = {naudts1996epistasis.pdf:Epistasis/Reference Papers/naudts1996epistasis.pdf:PDF}
}

@ARTICLE{FerranteVille2010AIR,
  author = {Neri, Ferrante and Tirronen, Ville},
  title = {Recent advances in differential evolution: a survey and experimental
	analysis},
  journal = {Artificial Intelligence Review},
  year = {2010},
  volume = {33},
  pages = {61-106},
  note = {10.1007/s10462-009-9137-2},
  abstract = {Differential Evolution (DE) is a simple and efficient optimizer, especially
	for continuous optimization. For these reasons DE has often been
	employed for solving various engineering problems. On the other hand,
	the DE structure has some limitations in the search logic, since
	it contains too narrow a set of exploration moves. This fact has
	inspired many computer scientists to improve upon DE by proposing
	modifications to the original algorithm. This paper presents a survey
	on DE and its recent advances. A classification, into two macro-groups,
	of the DE modifications is proposed here: (1) algorithms which integrate
	additional components within the DE structure, (2) algorithms which
	employ a modified DE structure. For each macro-group, four algorithms
	representative of the state-of-the-art in DE, have been selected
	for an in depth description of their working principles. In order
	to compare their performance, these eight algorithm have been tested
	on a set of benchmark problems. Experiments have been repeated for
	a (relatively) low dimensional case and a (relatively) high dimensional
	case. The working principles, differences and similarities of these
	recently proposed DE-based algorithms have also been highlighted
	throughout the paper. Although within both macro-groups, it is unclear
	whether there is a superiority of one algorithm with respect to the
	others, some conclusions can be drawn. At first, in order to improve
	upon the DE performance a modification which includes some additional
	and alternative search moves integrating those contained in a standard
	DE is necessary. These extra moves should assist the DE framework
	in detecting new promising search directions to be used by DE. Thus,
	a limited employment of these alternative moves appears to be the
	best option in successfully assisting DE. The successful extra moves
	are obtained in two ways: an increase in the exploitative pressure
	and the introduction of some randomization. This randomization should
	not be excessive though, since it would jeopardize the search. A
	proper increase in the randomization is crucial for obtaining significant
	improvements in the DE functioning. Numerical results show that,
	among the algorithms considered in this study, the most efficient
	additional components in a DE framework appear to be the population
	size reduction and the scale factor local search. Regarding the modified
	DE structures, the global and local neighborhood search and self-adaptive
	control parameter scheme, recently proposed in literature, seem to
	be the most promising modifications.},
  affiliation = {University of Jyväskylä Department of Mathematical Information Technology
	P.O. Box 35 Agora 40014 Jyväskylä Finland},
  issn = {0269-2821},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s10462-009-9137-2}
}

@ARTICLE{NomanIba2008TEVC,
  author = {Nasimul Noman. and Hitoshi Iba},
  title = {Accelerating Differential Evolution Using an Adaptive Local Search},
  journal = {Evolutionary Computation, IEEE Transactions on},
  year = {2008},
  volume = {12},
  pages = {107 -125},
  number = {1},
  month = {feb.},
  abstract = {We propose a crossover-based adaptive local search (LS) operation
	for enhancing the performance of standard differential evolution
	(DE) algorithm. Incorporating LS heuristics is often very useful
	in designing an effective evolutionary algorithm for global optimization.
	However, determining a single LS length that can serve for a wide
	range of problems is a critical issue. We present a LS technique
	to solve this problem by adaptively adjusting the length of the search,
	using a hill-climbing heuristic. The emphasis of this paper is to
	demonstrate how this LS scheme can improve the performance of DE.
	Experimenting with a wide range of benchmark functions, we show that
	the proposed new version of DE, with the adaptive LS, performs better,
	or at least comparably, to classic DE algorithm. Performance comparisons
	with other LS heuristics and with some other well-known evolutionary
	algorithms from literature are also presented.},
  doi = {10.1109/TEVC.2007.895272},
  file = {NomanIba2008TEVC.pdf:Evolutionary Computation/Differential Evolution/NomanIba2008TEVC.pdf:PDF},
  issn = {1089-778X},
  keywords = {crossover-based adaptive local search method;evolutionary algorithm;global
	optimization;hill-climbing heuristic;standard differential evolution
	algorithm;evolutionary computation;optimisation;search problems;}
}

@INPROCEEDINGS{Nomura97ananalysis,
  author = {Tatsuya Nomura},
  title = {An Analysis on Crossovers for Real Number Chromosomes in an Infinite
	Population Size},
  booktitle = {Proc. International Joint Conference on Artificial Intelligence (IJCAI-97)},
  year = {1997},
  pages = {936--941},
  file = {Nomura97ananalysis.pdf:EC_Course/Referenced Paper/Nomura97ananalysis.pdf:PDF}
}

@ARTICLE{JAIR2005Ortiz,
  author = {Domingo {Ortiz-Boyer} and C\'{e}sar {Herv\'{a}s-Mart\'{i}nez} and
	Nicol\'{a}s {Garc\'{i}a-Pedrajas}},
  title = {{CIXL2 - A crossover operator for evolutionary algorithms based on
	population features}},
  journal = {Journal of Artifical Intelligence Research},
  year = {2005},
  volume = {24},
  pages = {2005},
  __markedentry = {[chenwx]},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.7853},
  doi = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.7853},
  file = {JAIR2005Ortiz.pdf:Epistasis/Reference Papers/JAIR2005Ortiz.pdf:PDF}
}

@INCOLLECTION{SEAL2008PGGDLT,
  author = {Paperin, Greg and Green, David and Leishman, Tania},
  title = {Dual Phase Evolution and Self-organisation in Networks},
  booktitle = {Simulated Evolution and Learning},
  publisher = {Springer Berlin / Heidelberg},
  year = {2008},
  editor = {Li, Xiaodong and Kirley, Michael and Zhang, Mengjie and Green, David
	and Ciesielski, Vic and Abbass, Hussein and Michalewicz, Zbigniew
	and Hendtlass, Tim and Deb, Kalyanmoy and Tan, Kay and Branke, Jürgen
	and Shi, Yuhui},
  volume = {5361},
  series = {Lecture Notes in Computer Science},
  pages = {575-584},
  note = {10.1007/978-3-540-89694-4_58},
  abstract = {Dual Phase Evolution (DPE) is a widespread natural process in which
	complex systems adapt and self-organise by switching alternately
	between two phases: a phase of global interactions and a phase of
	local interactions. We show that in evolving networks of agents,
	DPE can give rise to a wide variety of topologies. In particular,
	it can lead to the spontaneous emergence of stabilising modular structure.},
  affiliation = {Monash University Faculty of Information Technology Clayton Campus,
	Building 63, Wellington Road Clayton 3800 Victoria Australia},
  file = {SEAL2008PGGDLT.pdf:Evolutionary Computation/Network Analysis/SEAL2008PGGDLT.pdf:PDF},
  url = {http://dx.doi.org/10.1007/978-3-540-89694-4_58}
}

@INPROCEEDINGS{Pelikan2010gecco,
  author = {Martin Pelikan},
  title = {NK landscapes, problem difficulty, and hybrid evolutionary algorithms},
  booktitle = {GECCO '10: Proceedings of the 12th annual conference on Genetic and
	evolutionary computation},
  year = {2010},
  pages = {665--672},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1830483.1830606},
  file = {Pelikan2010gecco.pdf:Epistasis/Reference Papers/Pelikan2010gecco.pdf:PDF},
  isbn = {978-1-4503-0072-8},
  location = {Portland, Oregon, USA}
}

@MISC{Pelikan2010geccoSlides,
  author = {Martin Pelikan},
  title = {NK landscapes, problem difficulty, and hybrid evolutionary algorithms},
  year = {2010},
  address = {New York, NY, USA},
  booktitle = {GECCO '10: Proceedings of the 12th annual conference on Genetic and
	evolutionary computation},
  doi = {http://medal.cs.umsl.edu/files/2010001.pdf},
  file = {Pelikan2010geccoSlides.pdf:Epistasis/Reference Papers/Pelikan2010geccoSlides.pdf:PDF},
  location = {Portland, Oregon, USA}
}

@ARTICLE{pelikan2000linkage,
  author = {Martin Pelikan and David E. Goldberg and Erick Cant{\'u}-Paz},
  title = {{Linkage problem, distribution estimation, and Bayesian networks}},
  journal = {Evolutionary Computation},
  year = {2000},
  volume = {8},
  pages = {340},
  number = {3},
  file = {pelikan2000linkage.pdf:Epistasis/Reference Papers/pelikan2000linkage.pdf:PDF},
  publisher = {MIT Press},
  review = {1. Propose a new modeling-based algorithm Bayesian Optimization Algorithm
	(BOA).
	
	2. BOA belongs to the class of EDA, it can discover the interactions
	during optimization.
	
	3. The upper limit for recognized order of interaction, says k, is
	define by user.
	
	
	Weakness:
	
	4. The prior information about the structure of problem, i.e., the
	highest possible order of interactions is required.}
}

@ARTICLE{pelikan2006performance,
  author = {Martin Pelikan and Kumara Sastry and Martin V. Butz and David E.
	Goldberg},
  title = {{Performance of evolutionary algorithms on random decomposable problems}},
  journal = {Parallel Problem Solving from Nature-PPSN IX},
  year = {2006},
  pages = {788--797},
  file = {pelikan2006performance.pdf:Epistasis/Reference Papers/pelikan2006performance.pdf:PDF},
  publisher = {Springer}
}

@INPROCEEDINGS{p851-pelikan,
  author = {Martin Pelikan and Kumara Sastry and David E. Goldberg and Martin
	V. Butz and Mark Hauschild},
  title = {{Performance of evolutionary algorithms on NK landscapes with nearest
	neighbor interactions and tunable overlap}},
  booktitle = {GECCO '09: Proceedings of the 11th Annual conference on Genetic and
	evolutionary computation},
  year = {2009},
  pages = {851--858},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1569901.1570018},
  file = {p851-pelikan.pdf:Epistasis/Reference Papers/p851-pelikan.pdf:PDF},
  isbn = {978-1-60558-325-9},
  location = {Montreal, Qu\'{e}bec, Canada}
}

@INPROCEEDINGS{rJADE,
  author = {Fei Peng and Ke Tang and Guoliang Chen and Xin Yao},
  title = {{Multi-Start JADE with Knowledge Transfer for Numerical Optimization}},
  booktitle = {IEEE Congress on Evolutionary Computation},
  year = {2009},
  pages = {1889--1895},
  publisher = {IEEE Press},
  file = {rJADE.pdf:Evolutionary Computation/JADE/Related PAPER/rJADE.pdf:PDF}
}

@ARTICLE{phillips1998language,
  author = {Patrick C. Phillips},
  title = {{The Language of Gene Interaction}},
  journal = {Genetics},
  year = {1998},
  volume = {149},
  pages = {1167},
  number = {3},
  file = {phillips1998language.pdf:Epistasis/Reference Papers/phillips1998language.pdf:PDF},
  publisher = {Genetics Soc America}
}

@BOOK{poli2008IntroGP,
  title = {{A field guide to genetic programming}},
  publisher = {Lulu Enterprises Uk Ltd},
  year = {2008},
  author = {Poli, R. and Langdon, W.B. and McPhee, N.F.},
  file = {poli2008IntroGP.pdf:Evolutionary Computation/poli2008IntroGP.pdf:PDF}
}

@INPROCEEDINGS{p353-popovici,
  author = {Popovici, Elena and De Jong, Kenneth},
  title = {The effects of interaction frequency on the optimization performance
	of cooperative coevolution},
  booktitle = {GECCO '06: Proceedings of the 8th annual conference on Genetic and
	evolutionary computation},
  year = {2006},
  pages = {353--360},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1143997.1144061},
  isbn = {1-59593-186-4},
  location = {Seattle, Washington, USA}
}

@INPROCEEDINGS{p507-popovici,
  author = {Elena Popovici and Kenneth {De Jong}},
  title = {Understanding cooperative co-evolutionary dynamics via simple fitness
	landscapes},
  booktitle = {GECCO '05: Proceedings of the 2005 conference on Genetic and evolutionary
	computation},
  year = {2005},
  pages = {507--514},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1068009.1068094},
  file = {p507-popovici.pdf:Evolutionary Computation/Large Scale Optimization/p507-popovici.pdf:PDF},
  isbn = {1-59593-010-8},
  location = {Washington DC, USA}
}

@PHDTHESIS{potter1997CCphdThesis,
  author = {Mitchell A. Potter},
  title = {{The Design and Analysis of a Computational Model of Cooperative
	Coevolution}},
  school = {George Mason University},
  year = {1997},
  file = {potter1997CCphdThesis.pdf:Evolutionary Computation/Large Scale Optimization/potter1997CCphdThesis.pdf:PDF}
}

@ARTICLE{CCEA,
  author = {Mitchell A. Potter and Kenneth Alan {De Jong}},
  title = {Cooperative Coevolution: architecture for evolving Coadapted subcomponents},
  journal = {Evolutionary Computation},
  year = {2000},
  volume = {8},
  pages = {1--29},
  number = {1},
  file = {CCEA.pdf:Evolutionary Computation/Large Scale Optimization/CCEA.pdf:PDF}
}

@INPROCEEDINGS{CCppsn,
  author = {Mitchell A. Potter and Kenneth Alan {De Jong}},
  title = {A Cooperative Coevolutionary Approach to Function Optimization},
  booktitle = {3rd Conf.\ on Parallel Problem Solving from Nature},
  year = {1994},
  volume = {2},
  pages = {249--257},
  file = {CCppsn.pdf:Evolutionary Computation/Large Scale Optimization/CCppsn.pdf:PDF}
}

@BOOK{DE2005APAGO,
  title = {Differential Evolution: A Practical Approach to Global Optimization},
  publisher = {Springer-Verlag},
  year = {2005},
  author = {Kenneth Price and Rainer Storn and Jouni A. Lampinen},
  isbn = {3-540-20950-6}
}

@CONFERENCE{cecSaDE,
  author = {Qin, AK and Suganthan, PN},
  title = {{Self-adaptive differential evolution algorithm for numerical optimization}},
  booktitle = {The 2005 IEEE Congress on Evolutionary Computation, 2005},
  year = {2005},
  pages = {1785--1791},
  file = {cecSaDE.pdf:Evolutionary Computation/Differential Evolution/cecSaDE.pdf:PDF}
}

@INPROCEEDINGS{RayYao2009cec,
  author = {Tapabrata Ray and Xin Yao},
  title = {{A Cooperative Coevolutionary Algorithm with Correlation Based Adaptive
	Variable Partitioning}},
  year = {2009},
  pages = {983 -989},
  month = {may.},
  abstract = {A cooperative coevolutionary algorithm (CCEA) is an extension to an
	evolutionary algorithm (EA); it employs a divide and conquer strategy
	to solve an optimization problem. In its basic form, a CCEA splits
	the variables of an optimization problem into multiple smaller subsets
	and evolves them independently in different subpopulations. The dynamics
	of a CCEA is far more complex than an EA and its performance can
	vary from good to bad depending on the separability of the optimization
	problem. This paper provides some insights into why CCEA in its basic
	form is not suitable for nonseparable problems and introduces a cooperative
	coevolutionary algorithm with correlation based adaptive variable
	partitioning (CCEA-AVP) to deal with such problems. The performance
	of CCEA-AVP is compared with CCEA and EA to highlight its benefits.
	CCEA-AVP offers the possibility to deal with problems where separability
	among variables might vary in different regions of the search space.},
  doi = {10.1109/CEC.2009.4983052},
  file = {RayYao2009cec.pdf:Evolutionary Computation/Large Scale Optimization/RayYao2009cec.pdf:PDF},
  journal = {IEEE Congress on Evolutionary Computation, 2009. CEC '09},
  keywords = {cooperative coevolutionary algorithm;correlation based adaptive variable
	partitioning;divide and conquer strategy;optimization problem;search
	space;divide and conquer methods;evolutionary computation;search
	problems;}
}

@ARTICLE{runarsson2000stochastic,
  author = {Runarsson, T.P. and Yao, X.},
  title = {{Stochastic ranking for constrained evolutionary optimization}},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2000},
  volume = {4},
  pages = {284--294},
  number = {3},
  file = {runarsson2000stochastic.pdf:Epistasis/Reference Papers/runarsson2000stochastic.pdf:PDF}
}

@ARTICLE{Salomon95reevaluatinggenetic,
  author = {Ralf Salomon},
  title = {Reevaluating Genetic Algorithm Performance under Coordinate Rotation
	of Benchmark Functions - A survey of some theoretical and practical
	aspects of genetic algorithms},
  journal = {BioSystems},
  year = {1995},
  volume = {39},
  pages = {263--278},
  file = {Salomon95reevaluatinggenetic.pdf:Epistasis/Reference Papers/Salomon95reevaluatinggenetic.pdf:PDF}
}

@BOOK{XinYaoEA,
  title = {Evolutionary Optimization},
  publisher = {Kluwer Academic Publishers Norwell},
  year = {2002},
  author = {R. Sarker and M. Mohammadian and X. Yao},
  address = {MA, USA},
  url = {http://scholar.google.com/scholar?hl=en&q=Evolutionary+Optimization+R.+Sarker+and+M.+Mohammadian+and+X.+Yao&btnG=Search&as_sdt=100000000000&as_ylo=&as_vis=0}
}

@BOOK{schwefel1993evolution,
  title = {{Evolution and Optimum Seeking: The Sixth Generation}},
  publisher = {John Wiley \& Sons, Inc. New York, NY, USA},
  year = {1993},
  author = {Schwefel, H.P.P.},
  file = {schwefel1993evolution.pdf:Evolutionary Computation/schwefel1993evolution.pdf:PDF}
}

@ARTICLE{segal1994SepFunc,
  author = {Uzi Segal},
  title = {{A sufficient condition for additively separable functions}},
  journal = {Journal of Mathematical Economics},
  year = {1994},
  volume = {23},
  pages = {295--303},
  number = {3},
  file = {segal1994SepFunc.pdf:Evolutionary Computation/JADE/Related PAPER/segal1994SepFunc.pdf:PDF},
  publisher = {Elsevier}
}

@ARTICLE{shi2005DECC-O,
  author = {Shi, Y. and Teng, H. and Li, Z.},
  title = {{Cooperative co-evolutionary differential evolution for function
	optimization}},
  journal = {Advances in Natural Computation},
  year = {2005},
  pages = {1080--1088},
  file = {shi2005DECC-O.pdf:Evolutionary Computation/Large Scale Optimization/shi2005DECC-O.pdf:PDF},
  publisher = {Springer}
}

@BOOK{SC1988NSFTBS,
  title = {Nonparametric Statistics for The Behavioral Sciences},
  publisher = {McGraw-Hill},
  year = {1988},
  author = {Siegel, S and {Castellan Jr.}John, N.},
  series = {Humanities/Social Sciences/Languages},
  address = {New York, USA}
}

@CONFERENCE{skellett2005NKruggedLS,
  author = {Skellett, B. and Cairns, B. and Geard, N. and Tonkes, B. and Wiles,
	J.},
  title = {{Maximally rugged NK landscapes contain the highest peaks}},
  booktitle = {Proceedings of the 2005 conference on Genetic and evolutionary computation},
  year = {2005},
  pages = {584},
  organization = {ACM},
  file = {skellett2005NKruggedLS.pdf:Evolutionary Computation/JADE/Related PAPER/Mergement Related Paper/skellett2005NKruggedLS.pdf:PDF}
}

@INPROCEEDINGS{Spears91onthe,
  author = {Villiam M. Spears and Kenneth A. De Jong},
  title = {On the virtues of parameterized uniform crossover},
  booktitle = {In Proceedings of the Fourth International Conference on Genetic
	Algorithms},
  year = {1991},
  pages = {230--236},
  file = {Spears91onthe.pdf:EC_Course/Referenced Paper/Spears91onthe.pdf:PDF}
}

@PHDTHESIS{spears1989NNandGA-NP,
  author = {William McDuff Spears},
  title = {{Using neural networks and genetic algorithms as heuristics for NP-complete
	problems}},
  school = {Citeseer},
  year = {1989},
  abstract = {Paradigms for using neural networks (NNs) and genetic algorithms (GAs)
	to
	
	heuristically solve boolean satisfiability (SAT) problems are presented.
	Results
	
	are presented for two-peak and false-peak SAT problems. Since SAT
	is NP-
	
	Complete, any other NP-Complete problem can be transformed into an
	equivalent
	
	SAT problem in polynomial time, and solved via either paradigm. This
	technique
	
	is illustrated for hamiltonian circuit (HC) problems.},
  file = {spears1989NNandGA-NP.pdf:Evolutionary Computation/spears1989NNandGA-NP.pdf:PDF}
}

@ARTICLE{storn1997DE,
  author = {Rainer Storn and Kenneth Price},
  title = {{Differential Evolution--A Simple and Efficient Heuristic for Global
	Optimization over Continuous Spaces}},
  journal = {Journal of Global Optimization},
  year = {1997},
  volume = {11},
  pages = {341--359},
  number = {4},
  file = {storn1997DE.pdf:Evolutionary Computation/Differential Evolution/storn1997DE.pdf:PDF},
  publisher = {Springer}
}

@CONFERENCE{streeter2004UpperBound,
  author = {Streeter, M.J.},
  title = {{Upper bounds on the time and space complexity of optimizing additively
	separable functions}},
  booktitle = {Genetic and Evolutionary Computation--GECCO 2004},
  year = {2004},
  pages = {186--197},
  organization = {Springer},
  file = {streeter2004UpperBound.pdf:Evolutionary Computation/JADE/Related PAPER/streeter2004UpperBound.pdf:PDF}
}

@ARTICLE{CEC2005benckmark,
  author = {Suganthan, P.N. and Hansen, N. and Liang, J.J. and Deb, K. and Chen,
	YP and Auger, A. and Tiwari, S.},
  title = {{Problem definitions and evaluation criteria for the CEC 2005 special
	session on real-parameter optimization}},
  year = {2005},
  volume = {2005005},
  file = {CEC2005benckmark.pdf:Evolutionary Computation/CEC2005benckmark.pdf:PDF},
  publisher = {Citeseer}
}

@MISC{TangCEC2008SSlsgo,
  author = {Ke Tang},
  title = {Summary of Results on CEC’08 Competition on Large Scale Global Optimization},
  year = {2008},
  file = {TangCEC2008SSlsgo.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/TangCEC2008SSlsgo.pdf:PDF},
  owner = {Wenxiang Chen},
  timestamp = {2010.08.20}
}

@TECHREPORT{cec2010lsgo,
  author = {Ke Tang and Xiaodong Li and Ponnuthurai Nagaratnam Suganthan and
	Zhenyu Yang and Thomas Weise},
  title = {Benchmark Functions for the {{CEC}}'2010 Special Session and Competition
	on Large Scale Global Optimization},
  institution = {NICAL, USTC},
  year = {2009},
  type = {{TR}},
  address = {Hefei, Anhui, China},
  note = {\url{http://nical.ustc.edu.cn/cec10ss.php}},
  file = {cec2010lsgo.pdf:Evolutionary Computation/lsgo_benchmark2010/cec2010lsgo.pdf:PDF}
}

@MISC{TLSYW2009BFFTCSSACOLSGO,
  author = {Ke Tang and Thomas Weise and Zhenyu Yang and Xiaodong Li and Ponnuthurai
	Nagaratnam Suganthan},
  title = {Results of the Large Scale Global Optimization Challenge at 2010
	IEEE World Congress on Computational Intelligence},
  year = {2010},
  file = {TLSYW2009BFFTCSSACOLSGO.pdf:Evolutionary Computation/lsgo_benchmark2010/TLSYW2009BFFTCSSACOLSGO.pdf:PDF},
  owner = {Wenxiang Chen},
  timestamp = {2010.08.20}
}

@TECHREPORT{tang2007CEC2008LSGO,
  author = {Ke Tang and Xin Yao and Suganthan, PN and MacNish, C. and Chen, YP
	and Chen, CM and Yang, Z.},
  title = {{Benchmark functions for the CEC’2008 special session and competition
	on large scale global optimization}},
  year = {2007},
  file = {tang2007CEC2008LSGO.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/tang2007CEC2008LSGO.pdf:PDF},
  journal = {Nature Inspired Computation and Applications Laboratory, USTC, China,
	Tech. Rep}
}

@CONFERENCE{tendresse2002partialRestart,
  author = {Tendresse, I. and Gottlieb, J. and Kao, O.},
  title = {{The effects of partial restarts in evolutionary search}},
  booktitle = {Artificial Evolution},
  year = {2002},
  pages = {117--127},
  organization = {Springer},
  file = {tendresse2002partialRestart.pdf:Evolutionary Computation/JADE/Related PAPER/tendresse2002partialRestart.pdf:PDF}
}

@CONFERENCE{tseng2008MTS,
  author = {Lin-Yu Tseng and Chun Chen},
  title = {{Multiple trajectory search for large scale global optimization}},
  booktitle = {IEEE Congress on Evolutionary Computation, 2008. CEC 2008.(IEEE World
	Congress on Computational Intelligence)},
  year = {2008},
  pages = {3052--3059},
  file = {tseng2008MTS.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/tseng2008MTS.pdf:PDF},
  review = {1. Include three kinds of local search, determine which one to apply
	after testing all of them for a few fitness.
	
	 1. Local Search 1 search along one dimension from the first dimension
	to the last dimension.
	
	 2. Local Search 2 is the similar to Local Search 2 except the it
	searchs along one-fourth of the all dimensions.
	
	 3. In local search 3, the evaluation of the objective function value
	is done after searching all the dimensions.
	
	
	2. Orthogonal Array (OA)
	
	Reduce the complexity of testing different level of configuration
	for several factors, which means we don't need to test all possible
	combination.
	
	
	3. Not clear about the OA, especially for SOA.}
}

@INPROCEEDINGS{ISDA2009VLGC,
  author = {Verma, Abhishek and Llor\`{a}, Xavier and Goldberg, David E. and
	Campbell, Roy H.},
  title = {Scaling Genetic Algorithms Using MapReduce},
  booktitle = {ISDA '09: Proceedings of the 2009 Ninth International Conference
	on Intelligent Systems Design and Applications},
  year = {2009},
  pages = {13--18},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  doi = {http://dx.doi.org/10.1109/ISDA.2009.181},
  isbn = {978-0-7695-3872-3}
}

@INCOLLECTION{Wang2009NIAO,
  author = {Yu Wang and Bin Li},
  title = {A Self-adaptive Mixed Distribution Based Uni-variate Estimation of
	Distribution Algorithm for Large Scale Global Optimization},
  booktitle = {Nature-Inspired Algorithms for Optimisation},
  publisher = {Springer Berlin / Heidelberg},
  year = {2009},
  editor = {Chiong, Raymond},
  volume = {193},
  series = {Studies in Computational Intelligence},
  pages = {171-198},
  note = {10.1007/978-3-642-00267-0_6},
  abstract = {Large scale global optimization (LSGO), which is highly needed for
	many scientific and engineering applications, is a very important
	and very difficult task in optimization domain. Various algorithms
	have been proposed to tackle this challenging problem, but the use
	of estimation of distribution algorithms (EDAs) to it is rare. This
	chapter aims at investigating the behavior and performances of uni-variate
	EDAs mixed with different kernel probability densities via fitness
	landscape analysis. Based on the analysis, a self-adaptive uni-variate
	EDA with mixed kernels (MUEDA) is proposed. To assess the effectiveness
	and efficiency of MUEDA, function optimization tasks with dimension
	scaling from 30 to 1500 are adopted. Compared to the recently published
	LSGO algorithms, MUEDA shows excellent convergence speed, final solution
	quality and dimensional scalability.},
  affiliation = {University of Science and Technology of China Nature Inspired Computation
	and Application Laboratory (NICAL)},
  file = {Wang2009NIAO.pdf:Evolutionary Computation/Large Scale Optimization/Wang2009NIAO.pdf:PDF},
  url = {http://dx.doi.org/10.1007/978-3-642-00267-0_6}
}

@INPROCEEDINGS{Wang2008cecss,
  author = {Yu Wang and Bin Li},
  title = {A restart univariate estimation of distribution algorithm: sampling
	under mixed Gaussian and L\'{e}vy probability distribution},
  year = {2008},
  pages = {3917 -3924},
  month = {jun.},
  doi = {10.1109/CEC.2008.4631330},
  file = {Wang2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/Wang2008cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {Levy probability distribution;continuous univariate marginal distribution
	algorithm;large scale global optimization problems;mixed Gaussian
	distribution;restart univariate estimation of distribution algorithm;sampling;standard
	deviation control strategy;Gaussian distribution;estimation theory;sampling
	methods;},
  review = {Key Points:
	
	1. Mixing L\`{e}vy distribution with Gaussian distribution.
	
	2. Standard Deviation Control Strategy(STDC), Set up a common threshold
	to maintain the diversity of population.
	
	3. Restart strategy is applied here as well.
	
	
	What I have learned:
	
	1.The Gaussian process is a typical example of stable process with
	finite second moment which would lead to a characteristic scale and
	the Gaussian behavior for the probability density through the central
	limit theorem.}
}

@CONFERENCE{weicker1999cec,
  author = {Karsten Weicker and Nicole Weicker},
  title = {{On the improvement of coevolutionary optimizers by learning variable
	interdependencies}},
  booktitle = {Proc. 1999 Congress on Evolutionary Computation (CEC'99). IEEE Press},
  year = {1999},
  pages = {1627--1632},
  file = {weicker1999cec.pdf:Evolutionary Computation/JADE/Related PAPER/Mergement Related Paper/weicker1999cec.pdf:PDF}
}

@BOOK{weise-global-2009,
  title = {Global Optimization Algorithms - Theory and Application},
  publisher = {Self-Published},
  year = {2009},
  author = {Thomas Weise},
  edition = {Second},
  month = may # {~1,},
  added-at = {2010-06-28T21:53:49.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/293936db6302cbf4cc1caf657906b1dc0/mhwombat},
  copyright = {Copyright (c) 2006-2009 Thomas Weise, licensed under GNU FDL},
  howpublished = {Online as e-book},
  institution = {University of Kassel, Distributed Systems Group},
  interhash = {a011ba76b2a6571b61aace60467278e6},
  intrahash = {93936db6302cbf4cc1caf657906b1dc0},
  keywords = {swarms hill_climbing java genetic_programming evolution annealing},
  language = {en},
  organization = {University of Kassel, Distributed Systems Group},
  url = {http://www.it-weise.de/}
}

@INPROCEEDINGS{whitley1989genitor,
  author = {Darrell Whitley},
  title = {The GENITOR algorithm and selection pressure: why rank-based allocation
	of reproductive trials is best},
  booktitle = {Proceedings of the third international conference on Genetic algorithms},
  year = {1989},
  pages = {116--121},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  acmid = {93169},
  file = {whitley1989genitor.pdf:Epistasis/Reference Papers/whitley1989genitor.pdf:PDF},
  isbn = {1-55860-006-3},
  location = {George Mason University, United States},
  numpages = {6},
  url = {http://portal.acm.org/citation.cfm?id=93126.93169}
}

@ARTICLE{Whitley96evaluatingevolutionary,
  author = {D. Whitley and K. Mathias and S. Rana and J. Dzubera},
  title = {Evaluating Evolutionary Algorithms},
  journal = {Artificial Intelligence},
  year = {1996},
  volume = {85},
  pages = {245--276}
}

@INPROCEEDINGS{MLCC2008cecss,
  author = {Zhenyu Yang and Ke Tang and Xin Yao},
  title = {Multilevel cooperative Coevolution for large scale optimization},
  booktitle = {IEEE Congress on Evolutionary Computation},
  year = {2008},
  pages = {1663--1670},
  publisher = {IEEE Press},
  file = {MLCC2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/MLCC2008cecss.pdf:PDF}
}

@ARTICLE{Yang-DECC-G,
  author = {Zhenyu Yang and Ke Tang and Xin Yao},
  title = {Large Scale Evolutionary Optimization Using Cooperative Coevolution},
  journal = {Information Sciences},
  year = {2008},
  volume = {178},
  pages = {2985--2999},
  number = {15},
  abstract = {Evolutionary algorithms (EAs) have been applied with success to many
	numerical and
	
	combinatorial optimization problems in recent years. However, they
	often lose their effec-
	
	tiveness and advantages when applied to large and complex problems,
	e.g., those with high
	
	dimensions. Although cooperative coevolution has been proposed as
	a promising frame-
	
	work for tackling high-dimensional optimization problems, only limited
	studies were
	
	reported by decomposing a high-dimensional problem into single variables
	(dimensions).
	
	Such methods of decomposition often failed to solve nonseparable problems,
	for which
	
	tight interactions exist among different decision variables. In this
	paper, we propose a
	
	new cooperative coevolution framework that is capable of optimizing
	large scale nonsep-
	
	arable problems. A random grouping scheme and adaptive weighting are
	introduced in
	
	problem decomposition and coevolution. Instead of conventional evolutionary
	algorithms,
	
	a novel differential evolution algorithm is adopted. Theoretical analysis
	is presented in this
	
	paper to show why and how the new framework can be effective for optimizing
	large non-
	
	separable problems. Extensive computational studies are also carried
	out to evaluate the
	
	performance of newly proposed algorithm on a large number of benchmark
	functions with
	
	up to 1000 dimensions. The results show clearly that our framework
	and algorithm are
	
	effective as well as efficient for large scale evolutionary optimisation
	problems. We are
	
	unaware of any other evolutionary algorithms that can optimize 1000-dimension
	nonsep-
	
	arable problems as effectively and efficiently as we have done.},
  file = {Yang-DECC-G.pdf:Evolutionary Computation/Large Scale Optimization/Yang-DECC-G.pdf:PDF}
}

@CONFERENCE{yang2008SaNSDE,
  author = {Yang, Z. and Tang, K. and Yao, X.},
  title = {{Self-adaptive differential evolution with neighborhood search}},
  booktitle = {IEEE Congress on Evolutionary Computation, 2008. CEC 2008.(IEEE World
	Congress on Computational Intelligence)},
  year = {2008},
  pages = {1110--1116},
  file = {yang2008SaNSDE.pdf:Evolutionary Computation/Differential Evolution/yang2008SaNSDE.pdf:PDF}
}

@INPROCEEDINGS{YangKeYao2007cec,
  author = {Zhenyu Yang and Ke Tang and Xin Yao},
  title = {Differential evolution for high-dimensional function optimization},
  year = {2007},
  pages = {3523 -3530},
  month = {sep.},
  abstract = {Most reported studies on differential evolution (DE) are obtained
	using low-dimensional problems, e.g., smaller than 100, which are
	relatively small for many real-world problems. In this paper we propose
	two new efficient DE variants, named DECC-I and DECC-II, for high-dimensional
	optimization (up to 1000 dimensions). The two algorithms are based
	on a cooperative coevolution framework incorporated with several
	novel strategies. The new strategies are mainly focus on problem
	decomposition and subcomponents cooperation. Experimental results
	have shown that these algorithms have superior performance on a set
	of widely used benchmark functions.},
  doi = {10.1109/CEC.2007.4424929},
  file = {YangKeYao2007cec.pdf:Evolutionary Computation/Large Scale Optimization/YangKeYao2007cec.pdf:PDF},
  journal = {IEEE Congress on Evolutionary Computation, 2007. CEC 2007},
  keywords = {cooperative coevolution framework;differential evolution;high-dimensional
	function optimization;optimisation;}
}

@ARTICLE{yang2008NSDE,
  author = {Yang, Z. and Yao, X. and He, J.},
  title = {{Making a difference to differential evolution}},
  journal = {Advances in Metaheuristics for Hard Optimization},
  year = {2008},
  pages = {397--414},
  file = {yang2008NSDE.pdf:Evolutionary Computation/Differential Evolution/yang2008NSDE.pdf:PDF},
  publisher = {Springer}
}

@INPROCEEDINGS{Yang-JACC-G,
  author = {Zhenyu Yang and Jinqiao Zhang and Ke Tang and Xin Yao and Arthur
	C. Sanderson},
  title = {An Adaptive Coevolutionary Differential Evolution Algorithm for Large-Scale
	Optimization},
  booktitle = {IEEE Congress on Evolutionary Computation},
  year = {2009},
  pages = {102--109},
  publisher = {IEEE Press},
  file = {Yang-JACC-G.pdf:Evolutionary Computation/Large Scale Optimization/Yang-JACC-G.pdf:PDF}
}

@ARTICLE{PIEEE1999EANN,
  author = {Xin Yao},
  title = {Evolving artificial neural networks},
  journal = {Proceedings of the IEEE},
  year = {1999},
  volume = {87},
  pages = {1423 -1447},
  number = {9},
  month = {sep.},
  abstract = {Learning and evolution are two fundamental forms of adaptation. There
	has been a great interest in combining learning and evolution with
	artificial neural networks (ANNs) in recent years. This paper: 1)
	reviews different combinations between ANNs and evolutionary algorithms
	(EAs), including using EAs to evolve ANN connection weights, architectures,
	learning rules, and input features; 2) discusses different search
	operators which have been used in various EAs; and 3) points out
	possible future research directions. It is shown, through a considerably
	large literature review, that combinations between ANNs and EAs can
	lead to significantly better intelligent systems than relying on
	ANNs or EAs alone},
  doi = {10.1109/5.784219},
  file = {PIEEE1999EANN.pdf:Evolutionary Computation/XinYao_Classic/PIEEE1999EANN.pdf:PDF},
  issn = {0018-9219},
  keywords = {connection weights;evolutionary algorithms;intelligent systems;learning;neural
	networks;search operators;genetic algorithms;learning (artificial
	intelligence);neural nets;search problems;technological forecasting;}
}

@ARTICLE{YaoTECFEP,
  author = {Xin Yao and Yong Liu and Guangming Lin},
  title = {{Evolutionary Programming Made Faster}},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {1999},
  volume = {3},
  pages = {82--102},
  number = {2},
  file = {YaoTecFEP.pdf:Evolutionary Computation/XinYao_Classic/YaoTecFEP.pdf:PDF}
}

@CONFERENCE{yao1996FEP,
  author = {Xin Yao and Yong},
  title = {{Fast evolutionary programming}},
  booktitle = {Proceedings of the Fifth Annual Conference on Evolutionary Programming},
  year = {1996},
  pages = {451--460},
  file = {yao1996FEP.pdf:Evolutionary Computation/XinYao_Classic/yao1996FEP.pdf:PDF}
}

@ARTICLE{yong2001CCMaS,
  author = {Yong, C.H. and Miikkulainen, R.},
  title = {{Cooperative coevolution of multi-agent systems}},
  journal = {University of Texas at Austin, Austin, TX},
  year = {2001},
  file = {yong2001CCMaS.pdf:Evolutionary Computation/JADE/Related PAPER/yong2001CCMaS.pdf:PDF},
  publisher = {Citeseer}
}

@CONFERENCE{zaharie2002ctrolParam,
  author = {Daniela Zaharie},
  title = {{Critical values for the control parameters of differential evolution
	algorithms}},
  booktitle = {Proceedings of MENDEL},
  year = {2002},
  pages = {62--67},
  file = {zaharie2002ctrolParam.pdf:Evolutionary Computation/zaharie2002ctrolParam.pdf:PDF}
}

@INPROCEEDINGS{Zamuda2008cecss,
  author = {Ale\v{x} Zamuda and Janez Brest and Borko Bo\v{x}kovi\'{c} and Viljem
	\v{Z}umer},
  title = {Large Scale Global Optimization using Differential Evolution with
	self-adaptation and cooperative co-evolution},
  year = {2008},
  pages = {3718 -3725},
  month = {jun.},
  abstract = {In this paper, an optimization algorithm is formulated and its performance
	assessment for large scale global optimization is presented. The
	proposed algorithm is named DEwSAcc and is based on Differential
	Evolution (DE) algorithm, which is a floating-point encoding evolutionary
	algorithm for global optimization over continuous spaces. The original
	DE is extended by log-normal self-adaptation of its control parameters
	and combined with cooperative co-evolution as a dimension decomposition
	mechanism. Experimental results are given for seven high-dimensional
	test functions proposed for the Special Session on Large Scale Global
	Optimization at 2008 IEEE World Congress on Computational Intelligence.},
  doi = {10.1109/CEC.2008.4631301},
  file = {Zamuda2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/Zamuda2008cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {cooperative co-evolution;differential evolution;large scale global
	optimization;self-adaptation;evolutionary computation;optimisation;},
  review = {1.Apply a self-adaptive strategy to control parameter $F$, $CR$
	
	
	A low-quality paper...}
}

@ARTICLE{zhang2009JADE,
  author = {Jingqiao Zhang and Arthur C. Sanderson},
  title = {{JADE: adaptive differential evolution with optional external archive}},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2009},
  volume = {13},
  pages = {945--958},
  number = {5},
  file = {zhang2009JADE.pdf:Evolutionary Computation/Differential Evolution/zhang2009JADE.pdf:PDF}
}

@INPROCEEDINGS{zhao2008cecss,
  author = {Zhao, S.Z. and Liang, J.J. and Suganthan, P.N. and Tasgetiren, M.F.},
  title = {Dynamic multi-swarm particle swarm optimizer with local search for
	Large Scale Global Optimization},
  year = {2008},
  pages = {3845 -3852},
  month = {jun.},
  abstract = {In this paper, the performance of dynamic multi-swarm particle swarm
	optimizer (DMS-PSO) on the set of benchmark functions provided for
	the CEC2008 Special Session on Large Scale optimization is reported.
	Different from the existing multi-swarm PSOs and local versions of
	PSO, the sub-swarms are dynamic and the sub-swarmspsila size is very
	small. The whole population is divided into a large number sub-swarms,
	these sub-swarms are regrouped frequently by using various regrouping
	schedules and information is exchanged among the particles in the
	whole swarm. The Quasi-Newton method is combined to improve its local
	searching ability.},
  doi = {10.1109/CEC.2008.4631320},
  file = {zhao2008cecss.pdf:Evolutionary Computation/LSGO.CEC08.Benchmark/Submitted Paper/zhao2008cecss.pdf:PDF},
  journal = {Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on
	Computational Intelligence). IEEE Congress on},
  keywords = {dynamic multi-swarm particle swarm optimizer;large scale global optimization;local
	search;quasi-newton method;regrouping schedules;sub-swarms;Newton
	method;particle swarm optimisation;search problems;}
}

@ARTICLE{ZhongLiuXueJiao2004TSMCB,
  author = {Weicai Zhong and Jing Liu and Mingzhi Xue and Licheng Jiao},
  title = {A multiagent genetic algorithm for global numerical optimization},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2004},
  volume = {34},
  pages = { 1128 - 1141},
  number = {2},
  month = {april},
  abstract = {In this paper, multiagent systems and genetic algorithms are integrated
	to form a new algorithm, multiagent genetic algorithm (MAGA), for
	solving the global numerical optimization problem. An agent in MAGA
	represents a candidate solution to the optimization problem in hand.
	All agents live in a latticelike environment, with each agent fixed
	on a lattice-point. In order to increase energies, they compete or
	cooperate with their neighbors, and they can also use knowledge.
	Making use of these agent-agent interactions, MAGA realizes the purpose
	of minimizing the objective function value. Theoretical analyzes
	show that MAGA converges to the global optimum. In the first part
	of the experiments, ten benchmark functions are used to test the
	performance of MAGA, and the scalability of MAGA along the problem
	dimension is studied with great care. The results show that MAGA
	achieves a good performance when the dimensions are increased from
	20-10,000. Moreover, even when the dimensions are increased to as
	high as 10,000, MAGA still can find high quality solutions at a low
	computational cost. Therefore, MAGA has good scalability and is a
	competent algorithm for solving high dimensional optimization problems.
	To the best of our knowledge, no researchers have ever optimized
	the functions with 10,000 dimensions by means of evolution. In the
	second part of the experiments, MAGA is applied to a practical case,
	the approximation of linear systems, with a satisfactory result.},
  doi = {10.1109/TSMCB.2003.821456},
  file = {ZhongLiuXueJiao2004TSMCB.pdf:Evolutionary Computation/Large Scale Optimization/ZhongLiuXueJiao2004TSMCB.pdf:PDF},
  issn = {1083-4419},
  keywords = {global numerical optimization; lattice like environment; linear system;
	multiagent genetic algorithm; genetic algorithms; numerical analysis;
	optimisation;}
}

@CONFERENCE{zhou2009CUDA-PSO,
  author = {You Zhou and Ying Tan},
  title = {{GPU-based parallel particle swarm optimization}},
  booktitle = {IEEE Congress on Evolutionary Computation, 2009. CEC'09},
  year = {2009},
  pages = {1493--1500},
  file = {zhou2009CUDA-PSO.pdf:Evolutionary Computation/GPU-based Algorithms/zhou2009CUDA-PSO.pdf:PDF}
}

@BOOK{AdvanDE,
  title = {Advances in Differential Evolution},
  publisher = {Springer},
  year = {2008},
  editor = {Uday K. Chakraborty},
  address = {Berlin},
  url = {http://portal.acm.org/citation.cfm?id=SERIES11878.1479611}
}

